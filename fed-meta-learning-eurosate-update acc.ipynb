{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:53:11.568429Z","iopub.status.busy":"2023-10-04T17:53:11.568109Z","iopub.status.idle":"2023-10-04T17:53:15.353172Z","shell.execute_reply":"2023-10-04T17:53:15.352303Z","shell.execute_reply.started":"2023-10-04T17:53:11.568402Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.transforms as transforms\n","import json\n","import os\n","import torch\n","from torchvision.datasets import VisionDataset\n","from torchvision.datasets.utils import check_integrity, download_and_extract_archive\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms, datasets,models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from copy import deepcopy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","batch_size = 8\n","augment= False\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","else:\n","  device = torch.device(\"cpu\")\n","print(device)\n","ways=5\n","shots=5"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:53:54.477577Z","iopub.status.busy":"2023-10-04T17:53:54.477260Z","iopub.status.idle":"2023-10-04T17:53:54.590920Z","shell.execute_reply":"2023-10-04T17:53:54.589596Z","shell.execute_reply.started":"2023-10-04T17:53:54.477550Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:53:54.593361Z","iopub.status.busy":"2023-10-04T17:53:54.593113Z","iopub.status.idle":"2023-10-04T17:54:40.562099Z","shell.execute_reply":"2023-10-04T17:54:40.561141Z","shell.execute_reply.started":"2023-10-04T17:53:54.593339Z"},"trusted":true},"outputs":[],"source":["train_dataset=datasets.ImageFolder(root=\"/home/siu856542507/zarif/FederatedMeta/2750\")\n","\n","#   transforms.Normalize([0.3444, 0.3803, 0.4078], [0.2027, 0.1369, 0.1156])\n","#         ]) weights=\"IMAGENET1K_V2\""]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:54:40.565252Z","iopub.status.busy":"2023-10-04T17:54:40.564550Z","iopub.status.idle":"2023-10-04T17:54:40.601994Z","shell.execute_reply":"2023-10-04T17:54:40.601063Z","shell.execute_reply.started":"2023-10-04T17:54:40.565216Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(24500, 2500)"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["from torch.utils.data import random_split\n","from torch.utils.data import Dataset\n","\n","random_seed = 22\n","torch.manual_seed(random_seed)\n","# val_size = 2500\n","test_size = 2500\n","train_size = len(train_dataset)  - test_size\n","\n","train_ds, test_ds  = random_split(train_dataset, [train_size, test_size])\n","len(train_ds), len(test_ds)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:54:40.604159Z","iopub.status.busy":"2023-10-04T17:54:40.603256Z","iopub.status.idle":"2023-10-04T17:54:40.613158Z","shell.execute_reply":"2023-10-04T17:54:40.612084Z","shell.execute_reply.started":"2023-10-04T17:54:40.604125Z"},"trusted":true},"outputs":[],"source":["train_transform = transforms.Compose([transforms.Resize(32),\n","                                      transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomRotation(10),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize([0.3444, 0.3803, 0.4078], [0.2027, 0.1369, 0.1156])\n","                                 ])\n","\n","test_transform = transforms.Compose([transforms.Resize(32),\n","                                 transforms.ToTensor(),\n","                                transforms.Normalize([0.3444, 0.3803, 0.4078], [0.2027, 0.1369, 0.1156])\n","                                    ])\n","class ApplyTransform(Dataset):\n","    def __init__(self, dataset, transform=None, target_transform=None):\n","        self.dataset = dataset\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        if transform is None and target_transform is None:\n","            print(\"Transform is not implemented :)\")\n","\n","    def __getitem__(self, idx):\n","        sample, target = self.dataset[idx]\n","        if self.transform is not None:\n","            sample = self.transform(sample)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        return sample, target\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","train_ds = ApplyTransform(train_ds, train_transform)\n","test_ds = ApplyTransform(test_ds,test_transform)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:54:40.632956Z","iopub.status.busy":"2023-10-04T17:54:40.632549Z","iopub.status.idle":"2023-10-04T17:54:40.651379Z","shell.execute_reply":"2023-10-04T17:54:40.650375Z","shell.execute_reply.started":"2023-10-04T17:54:40.632926Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import datasets\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","\n","# transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n","\n","\n","def non_iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle, shuffle_digits=False):\n","    assert(nb_nodes>0 and nb_nodes<=10)\n","\n","    digits=torch.arange(10) if shuffle_digits==False else torch.randperm(10, generator=torch.Generator().manual_seed(0))\n","    digits2=torch.arange(3) if shuffle_digits==False else torch.randperm(10, generator=torch.Generator().manual_seed(0))\n","\n","    digits = torch.cat((digits,digits2))\n","\n","    print(digits)\n","   \n","    digits_split=list()\n","    i=0\n","    for n in range(nb_nodes, 0, -1):\n","        inc=int((10-i)/n)\n","        digits_split.append(digits[i:i+3])\n","        print\n","        i+=inc\n","        print(inc)\n","        print(digits_split)\n","    \n","    print(digits_split)\n","    # load and shuffle nb_nodes*n_samples_per_node from the dataset\n","    loader = torch.utils.data.DataLoader(dataset,\n","                                        batch_size=nb_nodes*n_samples_per_node,\n","                                        shuffle=shuffle)\n","    dataiter = iter(loader)\n","    images_train_mnist, labels_train_mnist = next(dataiter)\n","\n","    data_splitted=list()\n","    for i in range(nb_nodes):\n","        idx=torch.stack([y_ == labels_train_mnist for y_ in digits_split[i]]).sum(0).bool() # get indices for the digits\n","        data_splitted.append(torch.utils.data.DataLoader(torch.utils.data.TensorDataset(images_train_mnist[idx], labels_train_mnist[idx]), batch_size=batch_size, shuffle=shuffle))\n","    print(data_splitted)\n","    return data_splitted\n","\n","def iid_split(dataset, nb_nodes, n_samples_per_node, batch_size, shuffle):\n","    # load and shuffle n_samples_per_node from the dataset\n","    loader = torch.utils.data.DataLoader(dataset,\n","                                        batch_size=n_samples_per_node,\n","                                        shuffle=shuffle)\n","    dataiter = iter(loader)\n","\n","    data_splitted=list()\n","\n","    for _ in range(nb_nodes):\n","        data_splitted.append(torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*(next(dataiter))), batch_size=batch_size, shuffle=shuffle))\n","\n","    return data_splitted\n","\n","\n","def  get_violence(type, n_samples_train, n_samples_test, n_clients, batch_size, shuffle,train_dataset , test_dataset):\n","\n","        if type==\"iid\":\n","            train=iid_split(train_dataset, n_clients, n_samples_train, batch_size, shuffle)\n","            test=iid_split(test_dataset, n_clients, n_samples_test, batch_size, shuffle)\n","        elif type==\"non_iid\":\n","            train=non_iid_split(train_dataset, n_clients, n_samples_train, batch_size, shuffle)\n","            test=non_iid_split(test_dataset, n_clients, n_samples_test, batch_size, shuffle)\n","        else:\n","            train=[]\n","            test=[]\n","\n","        return train, test\n","    \n","def  get_MNIST(type=\"iid\", n_samples_train=200, n_samples_test=100, n_clients=3, batch_size=25, shuffle=True):\n","    dataset_loaded_train = datasets.MNIST(\n","            root=\"./data\",\n","            train=True,\n","            download=True,\n","            transform=transforms.ToTensor()\n","    )\n","    dataset_loaded_test = datasets.MNIST(\n","            root=\"./data\",\n","            train=False,\n","            download=True,\n","            transform=transforms.ToTensor()\n","    )\n","\n","    if type==\"iid\":\n","        train=iid_split(dataset_loaded_train, n_clients, n_samples_train, batch_size, shuffle)\n","        test=iid_split(dataset_loaded_test, n_clients, n_samples_test, batch_size, shuffle)\n","    elif type==\"non_iid\":\n","        train=non_iid_split(dataset_loaded_train, n_clients, n_samples_train, batch_size, shuffle)\n","        test=non_iid_split(dataset_loaded_test, n_clients, n_samples_test, batch_size, shuffle)\n","    else:\n","        train=[]\n","        test=[]\n","\n","    return train, test\n","def plot_samples(data, channel:int, title=None, plot_name=\"\", n_examples =20):\n","\n","    n_rows = int(n_examples / 5)\n","    plt.figure(figsize=(1* n_rows, 1*n_rows))\n","    if title: plt.suptitle(title)\n","    X, y= data\n","    for idx in range(n_examples):\n","        \n","        ax = plt.subplot(n_rows, 5, idx + 1)\n","\n","        image = 255 - X[idx, channel].view((64,64))\n","        ax.imshow(image, cmap='gist_gray')\n","        ax.axis(\"off\")\n","    if plot_name!=\"\":plt.savefig(f\"plots/\"+plot_name+\".png\")\n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:54:40.652796Z","iopub.status.busy":"2023-10-04T17:54:40.652499Z","iopub.status.idle":"2023-10-04T17:55:09.843332Z","shell.execute_reply":"2023-10-04T17:55:09.842148Z","shell.execute_reply.started":"2023-10-04T17:54:40.652767Z"},"trusted":true},"outputs":[],"source":["mnist_iid_train_dls, mnist_iid_test_dls = get_violence(\"iid\",\n","    n_samples_train =500, n_samples_test=300, n_clients =3, \n","    batch_size = batch_size, shuffle =True,train_dataset=train_ds,test_dataset=test_ds)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:55:09.844964Z","iopub.status.busy":"2023-10-04T17:55:09.844627Z","iopub.status.idle":"2023-10-04T17:55:09.849734Z","shell.execute_reply":"2023-10-04T17:55:09.848597Z","shell.execute_reply.started":"2023-10-04T17:55:09.844931Z"},"trusted":true},"outputs":[],"source":["\n","# mnist_iid_train_dls, mnist_iid_test_dls = get_MNIST(\"iid\",\n","#     n_samples_train =500, n_samples_test=100, n_clients =10, \n","#     batch_size =batch_size, shuffle =True)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:55:09.866591Z","iopub.status.busy":"2023-10-04T17:55:09.865686Z","iopub.status.idle":"2023-10-04T17:55:13.627386Z","shell.execute_reply":"2023-10-04T17:55:13.626122Z","shell.execute_reply.started":"2023-10-04T17:55:09.866498Z"},"trusted":true},"outputs":[],"source":["# def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n","#     return nn.Sequential(\n","#         nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","#         nn.BatchNorm2d(out_channels),\n","#         nn.ReLU(),\n","#         nn.MaxPool2d(kernel_size=2, stride=1)\n","#     )\n","\n","# model = nn.Sequential(\n","#         conv_block(3, 64,3),\n","#         conv_block(64, 128,3),\n","#         conv_block(128, 256,3),\n","#         conv_block(256, 512,3),\n","#         nn.AdaptiveAvgPool2d((1,1)),\n","#         nn.Flatten(),\n","#         nn.Linear(512, 128),\n","#         nn.ReLU(),\n","#         nn.Dropout(0.3),\n","#         nn.Linear(128, 64),\n","#         nn.ReLU(),\n","#         nn.Dropout(0.3),\n","#         nn.Linear(64, ways),\n","#     )\n","        \n","\n","\n","# model=model.to(device)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:49:32.396342Z","iopub.status.busy":"2023-10-04T17:49:32.395645Z","iopub.status.idle":"2023-10-04T17:49:32.401247Z","shell.execute_reply":"2023-10-04T17:49:32.400156Z","shell.execute_reply.started":"2023-10-04T17:49:32.396297Z"},"trusted":true},"outputs":[],"source":["\n","model =models.resnet18(weights='IMAGENET1K_V1')\n","\n","# for param in model.parameters():\n","#     param.requires_grad = False\n","\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, ways)\n","\n","# # Transfer the model to device\n","model = model.to(device)\n","# Transfer the model to device\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:49:32.402846Z","iopub.status.busy":"2023-10-04T17:49:32.402564Z","iopub.status.idle":"2023-10-04T17:49:32.413697Z","shell.execute_reply":"2023-10-04T17:49:32.412495Z","shell.execute_reply.started":"2023-10-04T17:49:32.402824Z"},"trusted":true},"outputs":[],"source":["# class MnistResNet(nn.Module):\n","#   def __init__(self, in_channels=3):\n","#     super(MnistResNet, self).__init__()\n","\n","#     # Load a pretrained resnet model from torchvision.models in Pytorch\n","#     self.model = models.resnet50(pretrained=True)\n","\n","#     # Change the input layer to take Grayscale image, instead of RGB images. \n","#     # Hence in_channels is set as 1 or 3 respectively\n","#     # original definition of the first layer on the ResNet class\n","#     # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","#     self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","    \n","#     # Change the output layer to output 10 classes instead of 1000 classes\n","#     num_ftrs = self.model.fc.in_features\n","#     self.model.fc = nn.Linear(num_ftrs, 14)\n","\n","#   def forward(self, x):\n","#     return self.model(x)\n","\n","\n","# my_resnet = MnistResNet()\n","\n","# # input = torch.randn((16,1,244,244))\n","# # output = my_resnet(input)\n","\n","# model = MnistResNet().to(device)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:55:49.076210Z","iopub.status.busy":"2023-10-04T17:55:49.075857Z","iopub.status.idle":"2023-10-04T17:55:54.255686Z","shell.execute_reply":"2023-10-04T17:55:54.254765Z","shell.execute_reply.started":"2023-10-04T17:55:49.076182Z"},"trusted":true},"outputs":[],"source":["import learn2learn as l2l"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:55:54.258191Z","iopub.status.busy":"2023-10-04T17:55:54.257830Z","iopub.status.idle":"2023-10-04T17:55:54.298708Z","shell.execute_reply":"2023-10-04T17:55:54.297920Z","shell.execute_reply.started":"2023-10-04T17:55:54.258157Z"},"trusted":true},"outputs":[],"source":["training_tasks=[]\n","testing_tasks=[]\n","\n","for i in range (len(mnist_iid_train_dls)):\n","                \n","    train_dataset = l2l.data.MetaDataset(mnist_iid_train_dls[i].dataset)\n","#     valid_dataset = l2l.data.MetaDataset(valid_dataset)\n","\n","    train_transforms = [\n","        l2l.data.transforms.NWays(train_dataset, ways),\n","        l2l.data.transforms.KShots(train_dataset, 2*shots),\n","        l2l.data.transforms.LoadData(train_dataset),\n","        l2l.data.transforms.RemapLabels(train_dataset),\n","        l2l.data.transforms.ConsecutiveLabels(train_dataset),\n","    ]\n","    train_tasks = l2l.data.TaskDataset(train_dataset,\n","                                       task_transforms=train_transforms,\n","                                       num_tasks=1000)\n","    training_tasks.append(train_tasks)\n","#     valid_transforms = [\n","#         l2l.data.transforms.NWays(valid_dataset, ways),\n","#         l2l.data.transforms.KShots(valid_dataset, 2*shots),\n","#         l2l.data.transforms.LoadData(valid_dataset),\n","#         l2l.data.transforms.ConsecutiveLabels(train_dataset),\n","#         l2l.data.transforms.RemapLabels(valid_dataset),\n","#     ]\n","#     valid_tasks = l2l.data.TaskDataset(valid_dataset,\n","#                                        task_transforms=valid_transforms,\n","#                                        num_tasks=600)\n","   "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:55:54.300832Z","iopub.status.busy":"2023-10-04T17:55:54.300194Z","iopub.status.idle":"2023-10-04T17:55:54.323847Z","shell.execute_reply":"2023-10-04T17:55:54.322878Z","shell.execute_reply.started":"2023-10-04T17:55:54.300782Z"},"trusted":true},"outputs":[],"source":["for i in range (len(mnist_iid_test_dls)):\n","\n","    test_dataset = l2l.data.MetaDataset(mnist_iid_test_dls[i].dataset)\n","    test_transforms = [\n","        l2l.data.transforms.NWays(test_dataset, ways),\n","        l2l.data.transforms.KShots(test_dataset, 2*shots),\n","        l2l.data.transforms.LoadData(test_dataset),\n","        l2l.data.transforms.RemapLabels(test_dataset),\n","        l2l.data.transforms.ConsecutiveLabels(test_dataset),\n","    ]\n","    test_tasks = l2l.data.TaskDataset(test_dataset,\n","                                      task_transforms=test_transforms,\n","                                      num_tasks=600)\n","    testing_tasks.append(test_tasks)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:55:54.326428Z","iopub.status.busy":"2023-10-04T17:55:54.325905Z","iopub.status.idle":"2023-10-04T17:55:54.580780Z","shell.execute_reply":"2023-10-04T17:55:54.579644Z","shell.execute_reply.started":"2023-10-04T17:55:54.326391Z"},"trusted":true},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:57:11.973548Z","iopub.status.busy":"2023-10-04T17:57:11.973204Z","iopub.status.idle":"2023-10-04T17:57:11.994126Z","shell.execute_reply":"2023-10-04T17:57:11.993047Z","shell.execute_reply.started":"2023-10-04T17:57:11.973518Z"},"trusted":true},"outputs":[],"source":["# change loss calculation with simple loss and accuracy function not fast adapt \n","# meta model update on each iteration --> check parameters\n","def loss_classifier(predictions,labels):\n","    \n","    loss = nn.CrossEntropyLoss(reduction='mean')\n","    \n","    return loss(predictions ,labels)\n","\n","\n","def accuracy(predictions, targets):\n","    predictions = predictions.argmax(dim=1).view(targets.shape)\n","    acc= (predictions == targets).sum().float() / targets.size(0)\n","    return acc\n","\n","\n","def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n","\n","    data, labels = batch\n","    data, labels = data.to(device), labels.to(device)\n","    # Separate data into adaptation/evalutation sets\n","    adaptation_indices = torch.zeros(data.size(0)).byte()\n","    adaptation_indices[torch.arange(shots*ways) * 2] = 1\n","    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n","    evaluation_data, evaluation_labels = data[1 - adaptation_indices], labels[1 - adaptation_indices]\n","\n","    # Adapt the model\n","    for step in range(adaptation_steps):\n","        train_error = loss(learner(adaptation_data), adaptation_labels)\n","        train_error /= len(adaptation_data)\n","        learner.adapt(train_error)\n","    # Evaluate the adapted model\n","    predictions = learner(evaluation_data)\n","    valid_error = loss(predictions, evaluation_labels)\n","    valid_error /= len(evaluation_data)\n","    valid_accuracy = accuracy(predictions, evaluation_labels)\n","    \n","    return valid_error, valid_accuracy\n","def fast_adapt_metrics(batch, learner, loss, adaptation_steps, shots, ways, device):\n","\n","    data, labels = batch\n","    data, labels = data.to(device), labels.to(device)\n","    # Separate data into adaptation/evalutation sets\n","    adaptation_indices = torch.zeros(data.size(0)).byte()\n","    adaptation_indices[torch.arange(shots*ways) * 2] = 1\n","    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n","    evaluation_data, evaluation_labels = data[1 - adaptation_indices], labels[1 - adaptation_indices]\n","\n","    # Adapt the model\n","    for step in range(adaptation_steps):\n","        train_error = loss(learner(adaptation_data), adaptation_labels)\n","        # train_error /= len(adaptation_data)\n","        learner.adapt(train_error)\n","        \n","    # Evaluate the adapted model\n","    predictions = learner(evaluation_data)\n","    valid_error = loss(predictions, evaluation_labels)\n","    # valid_error /= len(evaluation_data)\n","    valid_accuracy = accuracy(predictions, evaluation_labels)\n","    # valid_accuracy = l2l.utils.accuracy(predictions, evaluation_labels)\n","\n","    return valid_error,valid_accuracy\n","def loss_dataset(model, dataset, loss_f):\n","    \"\"\"Compute the loss of `model` on `dataset`\"\"\"\n","    loss=0\n","    \n","    for idx,(features,labels) in enumerate(dataset):\n","        features = features.to(device)\n","        labels=labels.to(device)\n","        predictions= model(features)\n","        loss+=loss_f(predictions,labels)\n","    \n","    loss/=idx+1\n","    return loss\n","\n","\n","def accuracy_dataset(model, dataset):\n","    \n","    correct=0\n","    \n","    for features,labels in iter(dataset):\n","        features = features.to(device)\n","        labels=labels.to(device)\n","        predictions= model(features)\n","        _,predicted=predictions.max(1,keepdim=True)\n","        correct+=torch.sum(predicted.view(-1,1)==labels.view(-1, 1)).item()\n","    accuracy = 100*correct/len(dataset.dataset)\n","    return accuracy\n","\n","\n","def train_step(model, mu:int, optimizer, train_data, loss_f,loss,meta_batch_size,\n","        adaptation_steps):\n","    total_loss=0\n","    meta_train_error = 0.0\n","    meta_train_accuracy = 0.0\n","    meta_valid_error = 0.0\n","    meta_valid_accuracy = 0.0\n","    meta_test_error = 0.0\n","    meta_test_accuracy = 0.0\n","    for task in range(meta_batch_size):\n","        learner = model.clone()\n","        batch=train_data.sample()\n","\n","        evaluation_error,evaluation_accuracy= fast_adapt(batch,\n","                                                               learner,\n","                                                               loss,\n","                                                               adaptation_steps,\n","                                                               shots,\n","                                                               ways,\n","                                                               device)\n","        evaluation_error.backward()\n","        meta_train_error += evaluation_error.item()\n","        meta_train_accuracy += evaluation_accuracy.item()\n","        \n","#     for idx, (features,labels) in enumerate(train_data):\n","#         optimizer.zero_grad()\n","#         features = features.to(device)\n","#         predictions= model(features)\n","#         labels=labels.to(device)\n","#         loss=loss_f(predictions,labels)\n","#         loss+=mu/2*difference_models_norm_2(model,model_0)\n","#         total_loss+=loss\n","#         loss.backward()\n","#         optimizer.step()\n","#     print(\"meta accuracy: \", meta_train_accuracy/meta_batch_size)    \n","    return meta_train_error/meta_batch_size\n","\n","def meta_metrics(model,  optimizer, train_data, loss,meta_batch_size,\n","        adaptation_steps):\n"," \n","    losses=[]\n","    accuracy=[]\n","    for k in range(len(train_data)):\n","        total_loss=0\n","        meta_train_error = 0.0\n","        meta_train_accuracy = 0.0\n","        meta_valid_error = 0.0\n","        meta_valid_accuracy = 0.0\n","        meta_test_error = 0.0\n","        meta_test_accuracy = 0.0\n","        for task in range(meta_batch_size):\n","            learner = model.clone()\n","            batch= train_data[k].sample()\n","\n","            evaluation_error, evaluation_accuracy = fast_adapt_metrics(batch,\n","                                                                   learner,\n","                                                                   loss,\n","                                                                   adaptation_steps,\n","                                                                   shots,\n","                                                                   ways,\n","                                                                   device)\n","            meta_train_error += evaluation_error.item()\n","            meta_train_accuracy += evaluation_accuracy.item()\n","        losses.append(meta_train_error/meta_batch_size)\n","        accuracy.append(meta_train_accuracy/meta_batch_size)\n","        print(meta_train_accuracy/meta_batch_size)\n","    return losses,accuracy\n","\n","\n","\n","def local_learning(model, mu:float, optimizer, train_data, epochs:int, loss_f,loss,meta_batch_size,\n","        adaptation_steps):\n","    \n","#     model_0=deepcopy(model)\n","    \n","    for e in range(epochs):\n","        optimizer.zero_grad()\n","        local_loss=train_step(model,mu,optimizer,train_data,loss_f,loss,meta_batch_size,\n","        adaptation_steps)\n","\n","        for p in model.parameters():\n","            p.grad.data.mul_(1.0 / meta_batch_size)\n","        optimizer.step()\n","    return local_loss\n","\n","\n","def difference_models_norm_2(model_1, model_2):\n","    \"\"\"Return the norm 2 difference between the two model parameters\n","    \"\"\"\n","    \n","    tensor_1=list(model_1.parameters())\n","    tensor_2=list(model_2.parameters())\n","    \n","    norm=sum([torch.sum((tensor_1[i]-tensor_2[i])**2) \n","        for i in range(len(tensor_1))])\n","    \n","    return norm\n","\n","\n","def set_to_zero_model_weights(model):\n","    \"\"\"Set all the parameters of a model to 0\"\"\"\n","\n","    for layer_weigths in model.parameters():\n","        layer_weigths.data.sub_(layer_weigths.data)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:57:12.748668Z","iopub.status.busy":"2023-10-04T17:57:12.748299Z","iopub.status.idle":"2023-10-04T17:57:12.754279Z","shell.execute_reply":"2023-10-04T17:57:12.753307Z","shell.execute_reply.started":"2023-10-04T17:57:12.748626Z"},"trusted":true},"outputs":[],"source":["def average_models(model, clients_models_hist:list , weights:list):\n","\n","    \"\"\"Creates the new model of a given iteration with the models of the other\n","\n","    clients\"\"\"\n","    \n","    new_model=deepcopy(model)\n","    set_to_zero_model_weights(new_model)\n","\n","    for k,client_hist in enumerate(clients_models_hist):\n","        \n","        for idx, layer_weights in enumerate(new_model.parameters()):\n","\n","            contribution=client_hist[idx].data*weights[k]\n","            layer_weights.data.add_(contribution)\n","            \n","    return new_model\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Optimization Process"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:57:13.741108Z","iopub.status.busy":"2023-10-04T17:57:13.740405Z","iopub.status.idle":"2023-10-04T17:57:13.754506Z","shell.execute_reply":"2023-10-04T17:57:13.753617Z","shell.execute_reply.started":"2023-10-04T17:57:13.741066Z"},"trusted":true},"outputs":[],"source":["\n","def FedProx(model,training_sets:list, n_iter:int, testing_sets:list, lr=0.0005, meta_lr=0.01,  mu=0, \n","    file_name=\"test\", epochs=5,meta_batch_size=8,adaptation_steps=5,):\n","    \"\"\" all the clients are considered in this implementation of FedProx\n","    Parameters:\n","        - `model`: common structure used by the clients and the server\n","        - `training_sets`: list of the training sets. At each index is the \n","            training set of client \"index\"\n","        - `n_iter`: number of iterations the server will run\n","        - `testing_set`: list of the testing sets. If [], then the testing\n","            accuracy is not computed\n","        - `mu`: regularization term for FedProx. mu=0 for FedAvg\n","        - `epochs`: number of epochs each client is running\n","        - `lr`: learning rate of the optimizer\n","        - `decay`: to change the learning rate at each iteration\n"," \n","    \"\"\"\n","    loss_hist=[]\n","    acc_hist=[]\n","    loss_f=loss_classifier\n","    model= l2l.algorithms.MetaSGD(model, lr=meta_lr)\n","    local_optimizer = optim.Adam(model.parameters(),lr=lr)\n","    loss = nn.CrossEntropyLoss(reduction='mean')\n","    \n","    #Variables initialization\n","    K=len(training_sets) #number of clients\n","    n_samples=sum([len(db.dataset) for db in training_sets])\n","    weights=([len(db.dataset)/n_samples for db in training_sets])    \n","    losses,acc= meta_metrics(model,  local_optimizer, training_sets, loss,meta_batch_size,\n","        adaptation_steps)\n","    loss_hist.append(losses)\n","    acc_hist.append(acc)\n","    print(loss_hist)\n"," # CHANGE FOR META LEARNING\n","#     loss_hist=[[float(loss_dataset(model, dl, loss_f).detach()) \n","#         for dl in training_sets]]\n","#     print(loss_hist)\n","    \n","#     acc_hist=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n"," # CHANGE FOR META LEARNING\n","\n","    server_hist=[[tens_param.detach().cpu().numpy()\n","        for tens_param in list(model.parameters())]]\n","    models_hist = []\n","\n","    server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n","    server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n","    print(f'====> i: 0 Loss: {server_loss} Server Test Accuracy: {server_acc}')\n","    server_loss_list=[]\n","    server_accuracy_list=[]  \n","    \n","    for i in range(n_iter):\n","        \n","        clients_params=[]\n","        clients_models=[]\n","        clients_losses=[]\n","        \n","        for k in range(K):\n","\n","            local_model=deepcopy(model) #meta sgd model\n","#             local_optimizer=optim.SGD(local_model.parameters(), lr=0.001, momentum=0.9)\n","#             exp_lr_scheduler = lr_scheduler.StepLR(local_optimizer, step_size=7, gamma=0.1)\n","#             local_model= l2l.algorithms.MetaSGD(local_model, lr=meta_lr)\n","                \n","            local_optimizer = optim.Adam(local_model.parameters(),lr=lr) #meta sgd\n","            loss = nn.CrossEntropyLoss(reduction='mean')    \n","            local_loss=local_learning(local_model,mu,local_optimizer,\n","                training_sets[k],epochs,loss_f,loss,meta_batch_size,adaptation_steps)\n","            clients_losses.append(local_loss)\n","                \n","            #GET THE PARAMETER TENSORS OF THE MODEL\n","            list_params=list(local_model.parameters())\n","            list_params=[tens_param.detach() for tens_param in list_params]\n","            clients_params.append(list_params)    \n","            clients_models.append(deepcopy(local_model))\n","\n","            print(f\"{k}---local_loss--- {local_loss}\" )\n","        \n","        \n","        model = average_models(deepcopy(model), clients_params, \n","            weights=weights)\n","        models_hist.append(clients_models)\n","        \n","        #COMPUTE THE LOSS/ACCURACY OF THE DIFFERENT CLIENTS WITH THE NEW MODEL\n","        # Function TO CHANGE FOR META LEARNING\n","#         loss_hist+=[[float(loss_dataset(model, dl, loss_f).detach()) \n","#             for dl in training_sets]]\n","#         acc_hist+=[[accuracy_dataset(model, dl) for dl in testing_sets]]\n","        losses,acc= meta_metrics(model, local_optimizer, testing_sets, loss,meta_batch_size,\n","        adaptation_steps)\n","        loss_hist.append(losses)\n","        acc_hist.append(acc)\n","        server_loss=sum([weights[i]*loss_hist[-1][i] for i in range(len(weights))])\n","        server_acc=sum([weights[i]*acc_hist[-1][i] for i in range(len(weights))])\n","\n","        print(f'====> i: {i+1} Loss: {server_loss} Server Test Accuracy: {server_acc}')\n","        server_accuracy_list.append(server_acc)\n","        server_loss_list.append(server_loss)        \n","\n","        server_hist.append([tens_param.detach().cpu().numpy() \n","            for tens_param in list(model.parameters())])\n","    return model, loss_hist, acc_hist,server_accuracy_list,server_loss_list\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T17:57:14.300489Z","iopub.status.busy":"2023-10-04T17:57:14.300164Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.4299999885261059\n","0.5249999910593033\n","0.43499998562037945\n","[[1.431211456656456, 1.2827838063240051, 1.4114731550216675]]\n","====> i: 0 Loss: 1.3751561393340428 Server Test Accuracy: 0.4633333217352628\n","0---local_loss--- 0.054757604375481606\n","1---local_loss--- 0.051435474306344986\n","2---local_loss--- 0.04764404520392418\n","0.5799999870359898\n","0.5549999848008156\n","0.5049999840557575\n","====> i: 1 Loss: 1.3921604851881662 Server Test Accuracy: 0.5466666519641876\n","0---local_loss--- 0.020828805398195982\n","1---local_loss--- 0.017321402148809284\n","2---local_loss--- 0.0225201795110479\n","0.6549999825656414\n","0.6599999815225601\n","0.7199999764561653\n","====> i: 2 Loss: 1.5757484274605909 Server Test Accuracy: 0.6783333135147889\n","0---local_loss--- 0.004705465558799915\n","1---local_loss--- 0.005567531305132434\n","2---local_loss--- 0.003030992593267001\n","0.8499999642372131\n","0.8149999678134918\n","0.8149999864399433\n","====> i: 3 Loss: 0.83959392295219 Server Test Accuracy: 0.8266666394968827\n","0---local_loss--- 0.0020286806520743994\n","1---local_loss--- 0.0027678828992065974\n","2---local_loss--- 0.001394432450979366\n","0.7999999821186066\n","0.8899999707937241\n","0.8249999806284904\n","====> i: 4 Loss: 0.9929664639445642 Server Test Accuracy: 0.8383333111802737\n","0---local_loss--- 0.0026664308134058956\n","1---local_loss--- 0.0004829169611184625\n","2---local_loss--- 0.0014944510585337412\n","0.8699999824166298\n","0.7499999776482582\n","0.804999977350235\n","====> i: 5 Loss: 1.562682848853607 Server Test Accuracy: 0.8083333124717076\n","0---local_loss--- 0.0007692627259530127\n","1---local_loss--- 0.0027107243968202965\n","2---local_loss--- 0.0008497110538883135\n","0.8149999752640724\n","0.7599999755620956\n","0.8649999797344208\n","====> i: 6 Loss: 1.3305716228981812 Server Test Accuracy: 0.813333310186863\n","0---local_loss--- 0.0006283823367994046\n","1---local_loss--- 0.0009517398038951796\n","2---local_loss--- 0.0014303430089057656\n","0.804999977350235\n","0.8249999731779099\n","0.7599999755620956\n","====> i: 7 Loss: 1.7026669778861105 Server Test Accuracy: 0.7966666420300801\n","0---local_loss--- 0.0003821986756520346\n","1---local_loss--- 0.0010611441066430416\n","2---local_loss--- 0.0011372040116839344\n","0.9049999788403511\n","0.8099999874830246\n","0.8549999743700027\n","====> i: 8 Loss: 1.2385933411618073 Server Test Accuracy: 0.8566666468977927\n","0---local_loss--- 0.0015666595454604249\n","1---local_loss--- 0.0017565556590852793\n","2---local_loss--- 0.00038686243215124705\n","0.8649999797344208\n","0.7749999761581421\n","0.8949999883770943\n","====> i: 9 Loss: 1.364564045477247 Server Test Accuracy: 0.844999981423219\n","0---local_loss--- 0.0024076836671156343\n","1---local_loss--- 0.0028220261237947852\n","2---local_loss--- 0.0006576086016139016\n","0.8299999684095383\n","0.7649999707937241\n","0.8999999836087227\n","====> i: 10 Loss: 1.5160759097003997 Server Test Accuracy: 0.8316666409373283\n","0---local_loss--- 0.0006742646337443148\n","1---local_loss--- 0.0014142748696031049\n","2---local_loss--- 0.00039453601038985653\n","0.8349999710917473\n","0.7999999821186066\n","0.8549999669194221\n","====> i: 11 Loss: 1.7243821707864602 Server Test Accuracy: 0.8299999733765919\n","0---local_loss--- 0.0010436874326842371\n","1---local_loss--- 0.001142131253345724\n","2---local_loss--- 0.0008574858711654088\n","0.8249999731779099\n","0.8299999758601189\n","0.8649999722838402\n","====> i: 12 Loss: 1.3760453003148236 Server Test Accuracy: 0.8399999737739563\n","0---local_loss--- 0.0008254949370893883\n","1---local_loss--- 0.0008383065669477219\n","2---local_loss--- 0.000554660251509631\n","0.8599999845027924\n","0.7899999767541885\n","0.819999985396862\n","====> i: 13 Loss: 1.5343447806080803 Server Test Accuracy: 0.8233333155512809\n","0---local_loss--- 0.001225079983953492\n","1---local_loss--- 0.0001233576085724053\n","2---local_loss--- 0.0008483823112328537\n","0.8049999848008156\n","0.804999977350235\n","0.8849999830126762\n","====> i: 14 Loss: 1.5568117008854943 Server Test Accuracy: 0.8316666483879089\n","0---local_loss--- 0.0011876580729222042\n","1---local_loss--- 0.0008804937160675763\n","2---local_loss--- 0.0005924556708123419\n","0.8349999785423279\n","0.8349999785423279\n","0.8349999785423279\n","====> i: 15 Loss: 1.7304131258279083 Server Test Accuracy: 0.8349999785423279\n","0---local_loss--- 0.0004342089114288683\n","1---local_loss--- 0.0012901930367661407\n","2---local_loss--- 0.003241152888222132\n","0.9099999815225601\n","0.8349999785423279\n","0.8799999728798866\n","====> i: 16 Loss: 1.149406948427592 Server Test Accuracy: 0.8749999776482581\n","0---local_loss--- 0.0009965937750848752\n","1---local_loss--- 0.0008169295324478298\n","2---local_loss--- 0.0012183778544567758\n","0.8599999696016312\n","0.8149999678134918\n","0.8199999704957008\n","====> i: 17 Loss: 1.4651986546426388 Server Test Accuracy: 0.8316666359702745\n","0---local_loss--- 0.0007502686271436687\n","1---local_loss--- 0.0012930297743878327\n","2---local_loss--- 0.0005627891505355365\n","0.809999980032444\n","0.7749999687075615\n","0.8699999824166298\n","====> i: 18 Loss: 1.9778478741645813 Server Test Accuracy: 0.818333310385545\n","0---local_loss--- 0.0016972261241789965\n","1---local_loss--- 0.001976953892608435\n","2---local_loss--- 0.000555469539904152\n","0.8799999803304672\n","0.8399999737739563\n","0.9249999821186066\n","====> i: 19 Loss: 1.0218631749448832 Server Test Accuracy: 0.8816666454076767\n","0---local_loss--- 0.00018542029988566355\n","1---local_loss--- 0.0014559053895482066\n","2---local_loss--- 0.0030705983081134036\n","0.8399999812245369\n","0.8249999806284904\n","0.8299999758601189\n","====> i: 20 Loss: 1.4394830952369373 Server Test Accuracy: 0.831666645904382\n","0---local_loss--- 0.0006739441128047474\n","1---local_loss--- 0.002007839991392757\n","2---local_loss--- 0.0013555198861467943\n","0.8099999725818634\n","0.8249999806284904\n","0.7899999767541885\n","====> i: 21 Loss: 2.0521060563623905 Server Test Accuracy: 0.8083333099881806\n","0---local_loss--- 0.00023692179934187152\n","1---local_loss--- 0.0010906709089795186\n","2---local_loss--- 0.00029050803732388886\n","0.8149999678134918\n","0.8199999779462814\n","0.8349999710917473\n","====> i: 22 Loss: 1.7979488089962008 Server Test Accuracy: 0.8233333056171734\n","0---local_loss--- 0.0009179403214147897\n","1---local_loss--- 0.0001284746626879496\n","2---local_loss--- 0.0003180353892275889\n","0.7849999815225601\n","0.8149999752640724\n","0.804999977350235\n","====> i: 23 Loss: 2.122078200802207 Server Test Accuracy: 0.8016666447122891\n","0---local_loss--- 0.0002470655549586809\n","1---local_loss--- 0.001755762575726294\n","2---local_loss--- 0.0005512271618499653\n","0.8349999710917473\n","0.8049999698996544\n","0.8199999779462814\n","====> i: 24 Loss: 2.209932009379069 Server Test Accuracy: 0.8199999729792277\n","0---local_loss--- 0.00046942541894168244\n","1---local_loss--- 0.000689495016104047\n","2---local_loss--- 0.001820166176685234\n","0.8499999716877937\n","0.794999971985817\n","0.8199999704957008\n","====> i: 25 Loss: 1.8295587928344805 Server Test Accuracy: 0.821666638056437\n","0---local_loss--- 0.00036056004432794\n","1---local_loss--- 0.00017709399003251747\n","2---local_loss--- 0.00039643101445108186\n","0.8899999782443047\n","0.7949999831616879\n","0.8849999830126762\n","====> i: 26 Loss: 1.866482237332093 Server Test Accuracy: 0.8566666481395562\n","0---local_loss--- 0.001378818588023023\n","1---local_loss--- 0.00017332154675386846\n","2---local_loss--- 0.00016077898635558086\n","0.7999999821186066\n","0.8549999743700027\n","0.8499999791383743\n","====> i: 27 Loss: 2.3492508732306305 Server Test Accuracy: 0.8349999785423278\n","0---local_loss--- 0.0001860382377572023\n","1---local_loss--- 0.0008897543095827132\n","2---local_loss--- 0.0009313349485182698\n","0.8899999707937241\n","0.7999999783933163\n","0.8649999797344208\n","====> i: 28 Loss: 1.7360591236441298 Server Test Accuracy: 0.8516666429738203\n","0---local_loss--- 0.0005431181019730502\n","1---local_loss--- 0.0002910511281015715\n","2---local_loss--- 0.0005320997880744471\n","0.8699999824166298\n","0.8399999886751175\n","0.804999977350235\n","====> i: 29 Loss: 2.01068810664583 Server Test Accuracy: 0.8383333161473273\n","0---local_loss--- 6.247328985864442e-05\n","1---local_loss--- 0.000819856939415331\n","2---local_loss--- 0.002385401502124296\n","0.8449999615550041\n","0.8599999770522118\n","0.8999999612569809\n","====> i: 30 Loss: 1.7002363111823795 Server Test Accuracy: 0.8683332999547322\n"]}],"source":["import time\n","start_time = time.time()\n","\n","n_iter=40\n","# mnist_iid_train_dls, mnist_iid_test_dls\n","model_f, loss_hist_FA_iid, acc_hist_FA_iid,server_accuracy_list,server_loss_list = FedProx( model, training_tasks, \n","                                                                                           n_iter, testing_tasks,\n","   lr=0.0005, meta_lr=0.01, epochs =50,meta_batch_size=batch_size,\n","        adaptation_steps=5,)\n","end_time = time.time()\n","elapsed_time = end_time - start_time\n","print(\"Elapsed time: \", elapsed_time) \n","def plot_acc_loss(title:str, loss_hist:list, acc_hist:list):\n","    plt.figure()\n","    \n","    plt.suptitle(title)\n","\n","    plt.subplot(1,2,1)\n","    lines=plt.plot(loss_hist)\n","    plt.title(\"Loss\")\n","    plt.legend(lines,[\"C1\", \"C2\", \"C3\",\"C4\", \"C5\", \"C6\",\"C7\", \"C8\", \"C9\",\"C10\"])\n","\n","    plt.subplot(1,2,2)\n","    lines=plt.plot(acc_hist )\n","    plt.title(\"Accuracy\")\n","    plt.legend(lines,[\"C1\", \"C2\", \"C3\",\"C4\", \"C5\", \"C6\",\"C7\", \"C8\", \"C9\",\"C10\"])\n","    \n","\n","\n","import pickle\n","\n","\n","with open('acc-fedavo-lisa.pickle', 'wb') as handle:\n","    pickle.dump(acc_hist_FA_iid, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","with open('loss-fedavo-lisa.pickle', 'wb') as handle:\n","    pickle.dump(loss_hist_FA_iid, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","with open('server_acc_hist_fedavo-lisa.pickle', 'wb') as handle:\n","    pickle.dump(server_accuracy_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","with open('server_loss_list_fedavo-lisa.pickle', 'wb') as handle:\n","    pickle.dump(server_loss_list, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","    \n","plot_acc_loss(\"FedAvg MNIST-AVOA-IID-Epoch-5-Client-10\", loss_hist_FA_iid, acc_hist_FA_iid)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["n_samples=sum([len(db.dataset) for db in training_tasks])\n","weights=([len(db.dataset)/n_samples for db in training_tasks])    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"\\x1b[31m\\\"red\\\"\\x1b[0m\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
